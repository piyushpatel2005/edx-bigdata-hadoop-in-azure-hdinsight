Move data into cluster using Azure Storage Explorer. Copy all gzip files from Lab2/iislogs_gz into /data/logs

SSH to the cluster using Putty or SSH shell.
Look up the file in hadoop system
hadoop fs -ls /data/logs
# There should be 6 tar.gz files.

Launch Hive using hive or /usr/bin/hive

Create raw data table

CREATE TABLE rawlog
(log_date STRING,
 log_time STRING,
 c_ip STRING,
 cs_username STRING,
 s_ip STRING,
 s_port STRING,
 cs_method STRING,
 cs_uri_stem STRING,
 cs_uri_query STRING,
 sc_status STRING,
 sc_bytes INT,
 cs_bytes INT,
 time_taken INT,
 cs_user_agent STRING,
 cs_referrer STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ';

SELECT * FROM rawlog LIMIT 100;

This output contains comments in the first few columns. All these columns start with #.
So we can use that to clean the data.
Create another table to store cleandata

CREATE EXTERNAL TABLE cleanlog
(log_date DATE,
 log_time STRING,
 c_ip STRING,
 cs_username STRING,
 s_ip STRING,
 s_port STRING,
 cs_method STRING,
 cs_uri_stem STRING,
 cs_uri_query STRING,
 sc_status STRING,
 sc_bytes INT,
 cs_bytes INT,
 time_taken INT,
 cs_user_agent STRING,
 cs_referrer STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '
STORED AS TEXTFILE LOCATION '/data/cleanlog';

Load clean data using data that does not include # symbol in the first column

INSERT INTO TABLE cleanlog
SELECT * FROM rawlog
WHERE SUBSTR(log_date, 1, 1) <> '#';

Look up the clean data

SELECT * FROM cleanlog LIMIT 100;

Create a view for daily summary information

CREATE VIEW vDailySummary
AS 
SELECT log_date,
	COUNT(*) AS requests,
	SUM(cs_bytes) AS inbound_bytes,
	SUM(sc_bytes) AS outbound_bytes
FROM cleanlog
GROUP BY log_date;

SHOW TABLES;

This view appears as a table.

SELECT * FROM vDailySummary ORDER BY log_date;


Paritioned Data

CREATE EXTERNAL TABLE partitionedlog
(log_day int,
 log_time STRING,
 c_ip STRING,
 cs_username STRING,
 s_ip STRING,
 s_port STRING,
 cs_method STRING,
 cs_uri_stem STRING,
 cs_uri_query STRING,
 sc_status STRING,
 sc_bytes INT,
 cs_bytes INT,
 time_taken INT,
 cs_user_agent STRING,
 cs_referrer STRING)
PARTITIONED BY (log_year int, log_month int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '
STORED AS TEXTFILE LOCATION '/data/partitionedlog';

For dynamic paritioning, we need to set these properties.
SET hive.exec.dynamic.partition.mode=nonstrict;
SET hive.exec.dynamic.partition = true;

INSERT INTO TABLE partitionedlog PARTITION (log_year, log_month)
SELECT DAY(log_date), 
log_time, c_ip, cs_username, s_ip, s_port, cs_method, cs_uri_stem,
cs_uri_query, sc_status, sc_bytes, cs_bytes, time_taken, cs_user_agent, cs_referrer, YEAR(log_date), MONTH(log_date)
FROM rawlog
WHERE SUBSTR(log_date, 1, 1) <> '#';

Query will be faster if queried based on partitioned column

SELECT log_day, COUNT(*) AS page_hits
FROM partitionedlog
WHERE log_year=2008 AND log_month=6
GROUP BY log_day;

Exit from hive using 
exit;

hadoop fs -ls /data/partitionedlog
It contains folder with log_year=2008

Look inside that folder
hadoop fs -ls /data/partitionedlog/log_year=2008

hadoop fs -tail /data/partitionedlog/log_year=2008/log_month=6/000001_0
hadoop fs -text /data/partitionedlog/log_year=2008/log_month=6/000001_0 | head -5


Hadoop does not have head command directly.